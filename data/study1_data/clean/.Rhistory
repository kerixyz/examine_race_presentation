df_imputed[whichmissing,i] = mymean
}
else if (missing[i]) {
temp = table(df[,i])
mymode = names(temp)[length(temp)]
df_imputed[whichmissing,i] = mymode
}
}
return(df_imputed)
}
data = genRHC(outcome="death")
dim(data)
head(data)
data = genLalonde()
head(data)
table(data$z)
data$Label = data$z
data$ps = NULL
data$y = NULL
data$z = NULL
head(data)
write.csv("lalonde_all.csv",row.names=F,quote=F)
write.csv(data,"lalonde_all.csv",row.names=F,quote=F)
getwd()
data = genRHC(outcome="death")
head(data)
data$Label = data$z
data$y = NULL
head(data)
data$z = NULL
head(data)
write.csv(data,"rightheartcatherization_all.csv",row.names=F,quote=F)
### Function to one hot encode factors in data frame
one_hot_encode_data = function(data)
{
vars = colnames(data)
data_onehot = data[,1]  # just a hack, will be removed later
labeltemp = vars[1]   # just a hack
for (i in 1:ncol(data))
{
if (any(grepl("factor",class(data[,i])))) {
var = paste(vars[i],".",sep="")
colnames(data)[i] = var
f = as.formula(paste(labeltemp,"~",var,"-1",sep=""))
model = lm(f,data=data)
temp = model.matrix(model)
data_onehot = data.frame(data_onehot,temp)
print(paste("variable",var,"is factor, categories one-hot-encoded"))
}
else {
data_onehot = data.frame(data_onehot, subset(data,select=i))
}
}
data_onehot[,1] = NULL  # remove hack
return(data_onehot)
}
head(data)
goo = one_hot_encode_data(data)
head(data)
goo = one_hot_encode_data(data)
vars = colnames(data)
data_onehot = data[,1]  # just a hack, will be removed later
labeltemp = vars[1]
vars
for (i in 1:ncol(data))
{
if (any(grepl("factor",class(data[,i])))) {
var = paste(vars[i],".",sep="")
colnames(data)[i] = var
f = as.formula(paste(labeltemp,"~",var,"-1",sep=""))
model = lm(f,data=data)
temp = model.matrix(model)
data_onehot = data.frame(data_onehot,temp)
print(paste("variable",var,"is factor, categories one-hot-encoded"))
}
else {
data_onehot = data.frame(data_onehot, subset(data,select=i))
}
}
grepl("factor",class(data[,i]))
i
colnames(data)[i] = var
f = as.formula(paste(labeltemp,"~",var,"-1",sep=""))
model = lm(f,data=data)
temp = model.matrix(model)
f
### Function to one hot encode factors in data frame
one_hot_encode_data = function(data)
{
p = ncol(data)
data$temporary = rnorm(nrow(data))  # just a hack
vars = colnames(data)
data_onehot = data[,1]  # just a hack, will be removed later
for (i in 1:p)
{
if (any(grepl("factor",class(data[,i])))) {
var = paste(vars[i],".",sep="")
colnames(data)[i] = var
x = length(unique(data[,var]))
if (x<2) next
f = as.formula(paste("temporary~",var,"-1",sep=""))
model = lm(f,data=data)
temp = model.matrix(model)
data_onehot = data.frame(data_onehot,temp)
print(paste("variable",var,"is factor, categories one-hot-encoded"))
}
else {
data_onehot = data.frame(data_onehot, subset(data,select=i))
}
}
data_onehot[,1] = NULL  # remove hack
return(data_onehot)
}
goo = one_hot_encode_data(data)
head(goo)
dim(goo)
write.csv(data,"rightheartcatherization_all.csv",row.names=F,quote=F)
write.csv(goo,"Downloads/treeprototypes/data/rightheartcatherization_all.csv",row.names=F,quote=F)
dim(goo)
output = vector("numeric")
output = rbind(output,1:4)
output
output = rbind(output,1:4)
output
output =data.frame(output)
output
exp(0)
library(boostmtree)
install.packages("boostmtree")
datafolder = "/Users/sarah/Documents/Research/sleep data/blind/"
outputfolder = "/Users/sarah/Google Drive/sleep/output/"
# Load data
data = read.csv(paste(datafolder,"students_starttimes_outcomes_relevantcols.csv",sep=""),header=T)
datafolder = "/Users/sarah/Documents/Research/sleep data/blind/"
outputfolder = "/Users/sarah/Google Drive/sleep/output/"
# Load data
data = read.csv(paste(datafolder,"students_starttimes_outcomes_relevantcols.csv",sep=""),header=T)
library(dbarts)
logit
lirbary(arm)
logit(0.5)
library(arm)
logit(0.5)
logit(0.1)
logit(0.9)
logit(0.11)
logit(0.99)
logit(0.01)
goo = rnorm(100)
woo = runif(100)
y = goo + woo + rnorm(100)
data = data.frame(y,goo,woo)
mod = lm(y~.,data=data)
mod
predict(mod,newdata=data[,-y)
predict(mod,newdata=data[,-y])
x = data.frame(goo,woo)
predict(mod,newdata=x)
predict(mod,newdata=x,type="terms")
coef(fit)
coef(mod)
which(names(coef(mod))=="(Intercept)")
coef(mod)[1]
colnames(mo
)
goo = predict(mod,newdata=x,type="terms")
colnames(goo)
mod
print(mod)
temp = print(summary(mod
))
temp
save(temp,"delete.txt")
save(temp,"delete.txt")
?save
save(temp,file="delete.txt")
getwd()
capture.output(print(mod),
file = "delete.txt")
data(iris)
iris
head(iris)
adaboost
library(rsubgroup)
log(0.5/0.5)
R.Version()
devtools::install_github("rstudio/keras")
install.packages("keras")
keras::install_keras()
Imran.ahmed@quantumblack.com
install.packages("magick")
library(magick)
install.packages("cloudml")
library(cloudml)
gcloud_install()
library(keras)
library(cloudml)
library(magick)
gcloud_install()
# Subset data like ProPublica
keep = which((data$days_b_screening_arrest <= 30) & (data$days_b_screening_arrest >= -30) & (data$is_recid != -1) & (data$c_charge_degree != "O") & (data$score_text != 'N/A'))
library(cloudml)
gcloud_install()
apriori
install.packages("apriori")
?rgamma
library(grf)
# Generate data.
n = 2000; p = 10
X = matrix(rnorm(n*p), n, p)
X.test = matrix(0, 101, p)
X.test[,1] = seq(-2, 2, length.out = 101)
# Train a causal forest.
W = rbinom(n, 1, 0.4 + 0.2 * (X[,1] > 0))
Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
tau.forest = causal_forest(X, Y, W)
X = matrix(rnorm(n*p), n, p)
Y = X[,1] * rnorm(n)
X
Y
summary(Y)
Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
summary(Y)
ll.forest = local_linear_forest(X, Y)
local_linear_forest <- function(X, Y,
sample.fraction = 0.5,
mtry = NULL,
num.trees = 2000,
num.threads = NULL,
min.node.size = NULL,
honesty = TRUE,
honesty.fraction = NULL,
ci.group.size = 1,
alpha = NULL,
imbalance.penalty = NULL,
compute.oob.predictions = FALSE,
seed = NULL,
clusters = NULL,
samples_per_cluster = NULL,
tune.parameters = FALSE,
num.fit.trees = 10,
num.fit.reps = 100,
num.optimize.reps = 1000) {
validate_X(X)
if(length(Y) != nrow(X)) { stop("Y has incorrect length.") }
num.threads <- validate_num_threads(num.threads)
seed <- validate_seed(seed)
clusters <- validate_clusters(clusters, X)
samples_per_cluster <- validate_samples_per_cluster(samples_per_cluster, clusters)
honesty.fraction <- validate_honesty_fraction(honesty.fraction, honesty)
if (tune.parameters) {
tuning.output <- tune_regression_forest(X, Y,
num.fit.trees = num.fit.trees,
num.fit.reps = num.fit.reps,
num.optimize.reps = num.optimize.reps,
min.node.size = min.node.size,
sample.fraction = sample.fraction,
mtry = mtry,
alpha = alpha,
imbalance.penalty = imbalance.penalty,
num.threads = num.threads,
honesty = honesty,
honesty.fraction = honesty.fraction,
seed = seed,
clusters = clusters,
samples_per_cluster = samples_per_cluster)
tunable.params <- tuning.output$params
} else {
tunable.params <- c(
min.node.size = validate_min_node_size(min.node.size),
sample.fraction = validate_sample_fraction(sample.fraction),
mtry = validate_mtry(mtry, X),
alpha = validate_alpha(alpha),
imbalance.penalty = validate_imbalance_penalty(imbalance.penalty))
}
data <- create_data_matrices(X, Y)
outcome.index <- ncol(X) + 1
forest <- regression_train(data$default, data$sparse, outcome.index,
as.numeric(tunable.params["mtry"]),
num.trees,
num.threads,
as.numeric(tunable.params["min.node.size"]),
as.numeric(tunable.params["sample.fraction"]),
seed,
honesty,
coerce_honesty_fraction(honesty.fraction),
ci.group.size,
as.numeric(tunable.params["alpha"]),
as.numeric(tunable.params["imbalance.penalty"]),
clusters,
samples_per_cluster)
forest[["ci.group.size"]] <- ci.group.size
forest[["X.orig"]] <- X
forest[["Y.orig"]] <- Y
forest[["clusters"]] <- clusters
forest[["tunable.params"]] <- tunable.params
class(forest) <- c("regression_forest", "grf")
if (compute.oob.predictions) {
oob.pred <- predict(forest)
forest[["predictions"]] <- oob.pred$predictions
forest[["debiased.error"]] <- oob.pred$debiased.error
}
class(forest) = c("local_linear_forest", "grf")
forest
}
ll.forest = local_linear_forest(X, Y)
rm.packages("grf")
remove.packages("grf")
install.packages("grf")
install.packages("grf")
library(grf)
ll.forest = local_linear_forest(X, Y)
n = 2000; p = 10
X = matrix(rnorm(n*p), n, p)
X.test = matrix(0, 101, p)
X.test[,1] = seq(-2, 2, length.out = 101)
# Train a causal forest.
W = rbinom(n, 1, 0.4 + 0.2 * (X[,1] > 0))
Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
tau.forest = causal_forest(X, Y, W)
n = 2000; p = 10
X = matrix(rnorm(n*p), n, p)
X.test = matrix(0, 101, p)
X.test[,1] = seq(-2, 2, length.out = 101)
# Train a causal forest.
W = rbinom(n, 1, 0.4 + 0.2 * (X[,1] > 0))
Y = pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
tau.forest = causal_forest(X, Y, W)
ll.forest = local_linear_forest(X, Y)
local_linear_forest <- function(X, Y,
sample.fraction = 0.5,
mtry = NULL,
num.trees = 2000,
num.threads = NULL,
min.node.size = NULL,
honesty = TRUE,
honesty.fraction = NULL,
ci.group.size = 1,
alpha = NULL,
imbalance.penalty = NULL,
compute.oob.predictions = FALSE,
seed = NULL,
clusters = NULL,
samples_per_cluster = NULL,
tune.parameters = FALSE,
num.fit.trees = 10,
num.fit.reps = 100,
num.optimize.reps = 1000) {
if(length(Y) != nrow(X)) { stop("Y has incorrect length.") }
num.threads <- validate_num_threads(num.threads)
seed <- validate_seed(seed)
clusters <- validate_clusters(clusters, X)
samples_per_cluster <- validate_samples_per_cluster(samples_per_cluster, clusters)
honesty.fraction <- validate_honesty_fraction(honesty.fraction, honesty)
if (tune.parameters) {
tuning.output <- tune_regression_forest(X, Y,
num.fit.trees = num.fit.trees,
num.fit.reps = num.fit.reps,
num.optimize.reps = num.optimize.reps,
min.node.size = min.node.size,
sample.fraction = sample.fraction,
mtry = mtry,
alpha = alpha,
imbalance.penalty = imbalance.penalty,
num.threads = num.threads,
honesty = honesty,
honesty.fraction = honesty.fraction,
seed = seed,
clusters = clusters,
samples_per_cluster = samples_per_cluster)
tunable.params <- tuning.output$params
} else {
tunable.params <- c(
min.node.size = validate_min_node_size(min.node.size),
sample.fraction = validate_sample_fraction(sample.fraction),
mtry = validate_mtry(mtry, X),
alpha = validate_alpha(alpha),
imbalance.penalty = validate_imbalance_penalty(imbalance.penalty))
}
data <- create_data_matrices(X, Y)
outcome.index <- ncol(X) + 1
forest <- regression_train(data$default, data$sparse, outcome.index,
as.numeric(tunable.params["mtry"]),
num.trees,
num.threads,
as.numeric(tunable.params["min.node.size"]),
as.numeric(tunable.params["sample.fraction"]),
seed,
honesty,
coerce_honesty_fraction(honesty.fraction),
ci.group.size,
as.numeric(tunable.params["alpha"]),
as.numeric(tunable.params["imbalance.penalty"]),
clusters,
samples_per_cluster)
forest[["ci.group.size"]] <- ci.group.size
forest[["X.orig"]] <- X
forest[["Y.orig"]] <- Y
forest[["clusters"]] <- clusters
forest[["tunable.params"]] <- tunable.params
class(forest) <- c("regression_forest", "grf")
if (compute.oob.predictions) {
oob.pred <- predict(forest)
forest[["predictions"]] <- oob.pred$predictions
forest[["debiased.error"]] <- oob.pred$debiased.error
}
class(forest) = c("local_linear_forest", "grf")
forest
}
ll.forest = local_linear_forest(X, Y)
local_linear_forest <- function(X, Y,
sample.fraction = 0.5,
mtry = NULL,
num.trees = 2000,
num.threads = NULL,
min.node.size = NULL,
honesty = TRUE,
honesty.fraction = NULL,
ci.group.size = 1,
alpha = NULL,
imbalance.penalty = NULL,
compute.oob.predictions = FALSE,
seed = NULL,
clusters = NULL,
samples_per_cluster = NULL,
tune.parameters = FALSE,
num.fit.trees = 10,
num.fit.reps = 100,
num.optimize.reps = 1000) {
if(length(Y) != nrow(X)) { stop("Y has incorrect length.") }
if (tune.parameters) {
tuning.output <- tune_regression_forest(X, Y,
num.fit.trees = num.fit.trees,
num.fit.reps = num.fit.reps,
num.optimize.reps = num.optimize.reps,
min.node.size = min.node.size,
sample.fraction = sample.fraction,
mtry = mtry,
alpha = alpha,
imbalance.penalty = imbalance.penalty,
num.threads = num.threads,
honesty = honesty,
honesty.fraction = honesty.fraction,
seed = seed,
clusters = clusters,
samples_per_cluster = samples_per_cluster)
tunable.params <- tuning.output$params
} else {
tunable.params <- c(
min.node.size = validate_min_node_size(min.node.size),
sample.fraction = validate_sample_fraction(sample.fraction),
mtry = validate_mtry(mtry, X),
alpha = validate_alpha(alpha),
imbalance.penalty = validate_imbalance_penalty(imbalance.penalty))
}
data <- create_data_matrices(X, Y)
outcome.index <- ncol(X) + 1
forest <- regression_train(data$default, data$sparse, outcome.index,
as.numeric(tunable.params["mtry"]),
num.trees,
num.threads,
as.numeric(tunable.params["min.node.size"]),
as.numeric(tunable.params["sample.fraction"]),
seed,
honesty,
coerce_honesty_fraction(honesty.fraction),
ci.group.size,
as.numeric(tunable.params["alpha"]),
as.numeric(tunable.params["imbalance.penalty"]),
clusters,
samples_per_cluster)
forest[["ci.group.size"]] <- ci.group.size
forest[["X.orig"]] <- X
forest[["Y.orig"]] <- Y
forest[["clusters"]] <- clusters
forest[["tunable.params"]] <- tunable.params
class(forest) <- c("regression_forest", "grf")
if (compute.oob.predictions) {
oob.pred <- predict(forest)
forest[["predictions"]] <- oob.pred$predictions
forest[["debiased.error"]] <- oob.pred$debiased.error
}
class(forest) = c("local_linear_forest", "grf")
forest
}
ll.forest = local_linear_forest(X, Y)
getwd()
setwd("Dropbox/recidivism_replicate/data/clean")
getwd()
new = read.csv("master_2x2_bydefendant_new.csv",header=T)
head(new)
new = read.csv("master_2x2_bydefendant_old.csv",header=T)
head(new)
new = read.csv("master_2x2_bydefendant_new.csv",header=T)
old = read.csv("master_2x2_bydefendant_old.csv",header=T)
head(new)
head(new$turkerScore)
head(old$turkerScore)
master = read.csv("master_2x2.csv",header=T)
head(master)
head(new)
sum(master$defD==10008)
sum(master$defID==10008)
sum(master$defID==10009)
sum(master$defID==10044)
sum(master$defID==10044
)
sum(master$defId==7412
)
sum(master$defID==7412
)
sum(master$defID==10010)
